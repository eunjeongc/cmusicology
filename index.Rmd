---
title: "Portfolio for Computational Musicology 2024"
author: Sally Choi
output: 
  flexdashboard::flex_dashboard:
    orientation: columns
---

```{r setup, include=FALSE}
library(flexdashboard)
```

```{r}
library(tidyverse)
library(spotifyr)
library(compmus)
library(plotly)
library(gridExtra)
library(tidymodels)
library(ggdendro)
library(heatmaply)
```

Classification and Clustering
=================================================================================

Column {.tabset}
--------------------------------------------------------------------------------

### Chart A
```{r}
kpop_on <- get_playlist_audio_features("spotify", "1y6ywhJxaBAjAQ6Ybmg0kg")
kpop_mill <- get_playlist_audio_features("spotify", "2d9N60Iz8yzdsWLP1crgd2")
kpop <-
  bind_rows(
    kpop_on |> mutate(playlist = "ON!") |> slice_head(n = 20),
    kpop_mill |> mutate(playlist = "Millennium") |> slice_head(n = 20),
  ) |> 
  add_audio_analysis()
```

```{r}
kpop_features <-
  kpop |>  # For your portfolio, change this to the name of your corpus.
  mutate(
    playlist = factor(playlist),
    segments = map2(segments, key, compmus_c_transpose),
    pitches =
      map(
        segments,
        compmus_summarise, pitches,
        method = "mean", norm = "manhattan"
      ),
    timbre =
      map(
        segments,
        compmus_summarise, timbre,
        method = "mean",
      )
  ) |>
  mutate(pitches = map(pitches, compmus_normalise, "clr")) |>
  mutate_at(vars(pitches, timbre), map, bind_rows) |>
  unnest(cols = c(pitches, timbre))
```

```{r}
kpop_recipe <-
  recipe(
    playlist ~
      danceability +
      energy +
      loudness +
      speechiness +
      acousticness +
      instrumentalness +
      liveness +
      valence +
      tempo +
      duration +
      C + `C#|Db` + D + `D#|Eb` +
      E + `F` + `F#|Gb` + G +
      `G#|Ab` + A + `A#|Bb` + B +
      c01 + c02 + c03 + c04 + c05 + c06 +
      c07 + c08 + c09 + c10 + c11 + c12,
    data = kpop_features      # Use the same name as the previous block.
  ) |>
  step_center(all_predictors()) |>
  step_scale(all_predictors())      # Converts to z-scores.
  # step_range(all_predictors())    # Sets range to [0, 1].
```

```{r}
kpop_cv <- kpop_features |> vfold_cv(5)
```

```{r}
forest_model <-
  rand_forest() |>
  set_mode("classification") |> 
  set_engine("ranger", importance = "impurity")
kpop_forest <- 
  workflow() |> 
  add_recipe(kpop_recipe) |> 
  add_model(forest_model) |> 
  fit_resamples(
    kpop_cv, 
    control = control_resamples(save_pred = TRUE)
  )
```

```{r}
get_conf_mat <- function(fit) {
  outcome <- .get_tune_outcome_names(fit)
  fit |> 
    collect_predictions() |> 
    conf_mat(truth = outcome, estimate = .pred_class)
}  

get_pr <- function(fit) {
  fit |> 
    conf_mat_resampled() |> 
    group_by(Prediction) |> mutate(precision = Freq / sum(Freq)) |> 
    group_by(Truth) |> mutate(recall = Freq / sum(Freq)) |> 
    ungroup() |> filter(Prediction == Truth) |> 
    select(class = Prediction, precision, recall)
}  
```

```{r, results='hide'}
kpop_forest |> get_pr()
```

```{r}
workflow() |> 
  add_recipe(kpop_recipe) |> 
  add_model(forest_model) |> 
  fit(kpop_features) |> 
  pluck("fit", "fit", "fit") |>
  ranger::importance() |> 
  enframe() |> 
  mutate(name = fct_reorder(name, value)) |> 
  ggplot(aes(name, value)) + 
  geom_col() + 
  coord_flip() +
  theme_minimal() +
  labs(x = NULL, y = "Importance", title = "Feature Importance of Random Forest")
```

### Chart B
```{r}
kpop_mill <-
  get_playlist_audio_features("", "2d9N60Iz8yzdsWLP1crgd2") |>
  add_audio_analysis() |>
  mutate(
    segments = map2(segments, key, compmus_c_transpose),
    pitches =
      map(segments,
        compmus_summarise, pitches,
        method = "mean", norm = "manhattan"
      ),
    timbre =
      map(
        segments,
        compmus_summarise, timbre,
        method = "mean"
      )
  ) |>
  mutate(pitches = map(pitches, compmus_normalise, "clr")) |>
  mutate_at(vars(pitches, timbre), map, bind_rows) |>
  unnest(cols = c(pitches, timbre))
```

```{r}
kpop_on <-
  get_playlist_audio_features("", "1y6ywhJxaBAjAQ6Ybmg0kg") |>
  add_audio_analysis() |>
  mutate(
    segments = map2(segments, key, compmus_c_transpose),
    pitches =
      map(segments,
        compmus_summarise, pitches,
        method = "mean", norm = "manhattan"
      ),
    timbre =
      map(
        segments,
        compmus_summarise, timbre,
        method = "mean"
      )
  ) |>
  mutate(pitches = map(pitches, compmus_normalise, "clr")) |>
  mutate_at(vars(pitches, timbre), map, bind_rows) |>
  unnest(cols = c(pitches, timbre))
```

```{r}
kpop <-
  bind_rows(
    kpop_mill |> mutate(category = "Millennium"),
    kpop_on |> mutate(category = "ON!")
  )
```

```{r}
kpop_juice <-
  recipe(
    track.name ~
      duration +
      energy +
      acousticness +
      `C#|Db` + G +
      `G#|Ab` + `A#|Bb` + B +
      c02 + c03 + c05 +
      c08 + c12,
    data = kpop
  ) |>
  step_center(all_predictors()) |>
  step_scale(all_predictors()) |> 
  # step_range(all_predictors()) |> 
  prep(kpop |> mutate(track.name = str_trunc(track.name, 20))) |>
  juice() |>
  column_to_rownames("track.name")
```

```{r}
kpop_dist <- dist(kpop_juice, method = "euclidean")
```

```{r}
kpop_dist |> 
  hclust(method = "complete") |> # Try single, average, and complete.
  dendro_data() |>
  ggdendrogram() + 
  theme(text = element_text(size = 3)) + 
  ggtitle("Dendrogram of K-pop ON! and K-pop Millennium")
```

### Chart C
```{r}
kpop_features |>
  ggplot(aes(x = duration, y = acousticness, colour = playlist, size = energy)) +
  geom_point(alpha = 0.8) +
  scale_color_viridis_d() +
  labs(
    x = "Duration",
    y = "Acousticness",
    size = "Energy",
    colour = "Playlist",
    title = "Relationship between Duration and Acousticness: K-pop Millennium vs. K-pop ON"
  )
```


Column
--------------------------------------------------------------------------------

### What are the key features that distinguish K-pop songs from different eras?

In my initial exploration, I used the clustering algorithm to examine its ability to accurately cluster K-pop songs in their corresponding era. However, since each playlist contains 100 tracks, the resulting dendrogram displayed 200 tracks, posing challenges in the interpretation. Consequently, I tried the classification algorithm to identify key features that the algorithm uses to classify the different eras. Through this analysis, I identified duration, G, c12, A#, C05, C02, B, C08, C#, C03, G#, energy, and acousticness as the top 13 features for classification. With this insight, I re-ran the clustering algorithm solely focusing on these features, which resulted in a more refined dendrogram. Despite the inherent complexity of the dendrogram due to the large number of tracks, it became evident that the clustering algorithm performed more effectively when leveraging the key features identified by the classification algorithm. An alternative way to visualize the significance of key features is shown in Chart C, which uses the following features: acousticness, duration, and energy. Notably, it reveals a distinct clustering pattern where tracks from the millennium era are grouped with higher duration, while recent releases tend to have shorter durations. Such depiction offers a clear and insightful perspective on how these features contribute to the differentiation of K-pop songs across different periods.


Tempogram
=================================================================================


Column {.tabset}
--------------------------------------------------------------------------------

### Chart A

```{r}
jungkook_tempo <- get_tidy_audio_analysis("2KslE17cAJNHTsI2MI0jb2")
```

```{r}
jungkook_tempo |>
  tempogram(window_size = 8, hop_size = 1, cyclic = TRUE) |>
  ggplot(aes(x = time, y = bpm, fill = power)) +
  geom_raster() +
  scale_fill_viridis_c(guide = "none") +
  labs(x = "Time (s)", y = "Tempo (BPM)", title = "Tempogram of 'Standing Next to You (2023)'") +
  theme_classic()
```

### Chart B

```{r}
wgirls_tempo <- get_tidy_audio_analysis("2UuKZDzjZ7cVMOvpmuAMYj")
```

```{r}
wgirls_tempo |>
  tempogram(window_size = 8, hop_size = 1, cyclic = TRUE) |>
  ggplot(aes(x = time, y = bpm, fill = power)) +
  geom_raster() +
  scale_fill_viridis_c(guide = "none") +
  labs(x = "Time (s)", y = "Tempo (BPM)", title = "Tempogram of 'Tell Me (2007)'") +
  theme_classic()
```

### Chart C

```{r}
jungkook_remix_tempo <- get_tidy_audio_analysis("0VPFT123HKoQ2J6ipeDcI1")
```

```{r}
jungkook_remix_tempo |>
  tempogram(window_size = 8, hop_size = 1, cyclic = TRUE) |>
  ggplot(aes(x = time, y = bpm, fill = power)) +
  geom_raster() +
  scale_fill_viridis_c(guide = "none") +
  labs(x = "Time (s)", y = "Tempo (BPM)", title = "Tempogram of 'Standing Next to You Remix (2023)'") +
  theme_classic()
```


Column 
--------------------------------------------------------------------------------

### Does the tempo of K-pop tracks match the diverse range of genres it references?

The tempo variation in 'Standing Next to You (Chart A)' exhibits greater diversity, with fluctuations between 105 bpm, 135 bpm, and 145 bpm. [Other sources](https://songbpm.com/@jung-kook/standing-next-to-you-X_F-hVBltl) confirm that the track's tempo is 106 bpm, aligning with Spotify's data. Such tempo variation reflects the dynamic nature of the jazz-funk genre, typically ranging between [90-110 bpm.](https://splice.com/blog/genre-focus-series-funk-with-splice-sounds/#:~:text=Tempo%20%26%20Meter%3A%20Most%20funk%20songs,dirty%20on%20the%20dance%20floor.) Conversely, 'Tell Me (Chart B)' maintains a consistent tempo of 125 bpm throughout, as confirmed by [additional source](https://songbpm.com/@wonder-girls/tell-me) indicating 127bpm. This tempo corresponds to the steady pace characteristic of disco, which typically centers around [120 bpm.](esounds.html#:~:text=The%20basic%20tempo%20of%20disco,the%20gaps%20between%20the%20beats.)

While there aren't significant differences in tempo between 'Standing Next to You' and the its remixed version featuring USHER (Chart C), fluctuations occur notably around 110 and 150 seconds, coinciding with the initiation of USHER's vocal. His segment, characterized by a softer, laid-back vibe, temporarily reduces the tempo to approximately 90 bpm before and after the 150-second mark. Apart from these minor fluctuations, the main tempo remains consistent around 105 bpm, as indicated by a strong, straight line.

Chordogram 
=================================================================================


Column {.tabset}
--------------------------------------------------------------------------------

### Chart A
```{r}
circshift <- function(v, n) {
  if (n == 0) v else c(tail(v, n), head(v, -n))
}

#      C     C#    D     Eb    E     F     F#    G     Ab    A     Bb    B
major_chord <-
  c(   1,    0,    0,    0,    1,    0,    0,    1,    0,    0,    0,    0)
minor_chord <-
  c(   1,    0,    0,    1,    0,    0,    0,    1,    0,    0,    0,    0)
seventh_chord <-
  c(   1,    0,    0,    0,    1,    0,    0,    1,    0,    0,    1,    0)

major_key <-
  c(6.35, 2.23, 3.48, 2.33, 4.38, 4.09, 2.52, 5.19, 2.39, 3.66, 2.29, 2.88)
minor_key <-
  c(6.33, 2.68, 3.52, 5.38, 2.60, 3.53, 2.54, 4.75, 3.98, 2.69, 3.34, 3.17)

chord_templates <-
  tribble(
    ~name, ~template,
    "Gb:7", circshift(seventh_chord, 6),
    "Gb:maj", circshift(major_chord, 6),
    "Bb:min", circshift(minor_chord, 10),
    "Db:maj", circshift(major_chord, 1),
    "F:min", circshift(minor_chord, 5),
    "Ab:7", circshift(seventh_chord, 8),
    "Ab:maj", circshift(major_chord, 8),
    "C:min", circshift(minor_chord, 0),
    "Eb:7", circshift(seventh_chord, 3),
    "Eb:maj", circshift(major_chord, 3),
    "G:min", circshift(minor_chord, 7),
    "Bb:7", circshift(seventh_chord, 10),
    "Bb:maj", circshift(major_chord, 10),
    "D:min", circshift(minor_chord, 2),
    "F:7", circshift(seventh_chord, 5),
    "F:maj", circshift(major_chord, 5),
    "A:min", circshift(minor_chord, 9),
    "C:7", circshift(seventh_chord, 0),
    "C:maj", circshift(major_chord, 0),
    "E:min", circshift(minor_chord, 4),
    "G:7", circshift(seventh_chord, 7),
    "G:maj", circshift(major_chord, 7),
    "B:min", circshift(minor_chord, 11),
    "D:7", circshift(seventh_chord, 2),
    "D:maj", circshift(major_chord, 2),
    "F#:min", circshift(minor_chord, 6),
    "A:7", circshift(seventh_chord, 9),
    "A:maj", circshift(major_chord, 9),
    "C#:min", circshift(minor_chord, 1),
    "E:7", circshift(seventh_chord, 4),
    "E:maj", circshift(major_chord, 4),
    "G#:min", circshift(minor_chord, 8),
    "B:7", circshift(seventh_chord, 11),
    "B:maj", circshift(major_chord, 11),
    "D#:min", circshift(minor_chord, 3)
  )

key_templates <-
  tribble(
    ~name, ~template,
    "Gb:maj", circshift(major_key, 6),
    "Bb:min", circshift(minor_key, 10),
    "Db:maj", circshift(major_key, 1),
    "F:min", circshift(minor_key, 5),
    "Ab:maj", circshift(major_key, 8),
    "C:min", circshift(minor_key, 0),
    "Eb:maj", circshift(major_key, 3),
    "G:min", circshift(minor_key, 7),
    "Bb:maj", circshift(major_key, 10),
    "D:min", circshift(minor_key, 2),
    "F:maj", circshift(major_key, 5),
    "A:min", circshift(minor_key, 9),
    "C:maj", circshift(major_key, 0),
    "E:min", circshift(minor_key, 4),
    "G:maj", circshift(major_key, 7),
    "B:min", circshift(minor_key, 11),
    "D:maj", circshift(major_key, 2),
    "F#:min", circshift(minor_key, 6),
    "A:maj", circshift(major_key, 9),
    "C#:min", circshift(minor_key, 1),
    "E:maj", circshift(major_key, 4),
    "G#:min", circshift(minor_key, 8),
    "B:maj", circshift(major_key, 11),
    "D#:min", circshift(minor_key, 3)
  )
```

```{r}
jungkook_chord <-
  get_tidy_audio_analysis("2KslE17cAJNHTsI2MI0jb2") |>
  compmus_align(sections, segments) |>
  select(sections) |>
  unnest(sections) |>
  mutate(
    pitches =
      map(segments,
        compmus_summarise, pitches,
        method = "mean", norm = "manhattan"
      )
  )

```

```{r}
jungkook_chord |> 
  compmus_match_pitch_template(
    chord_templates,         # Change to chord_templates if desired
    method = "euclidean",  # Try different distance metrics
    norm = "manhattan"     # Try different norms
  ) |>
  ggplot(
    aes(x = start + duration / 2, width = duration, y = name, fill = d)
  ) +
  geom_tile() +
  scale_fill_viridis_c(guide = "none") +
  theme_minimal() +
  labs(x = "Time (s)", y = "", title = "Chordogram of 'Standing Next to You (2023)'")
```

### Chart B
```{r}
wgirls_chord <-
  get_tidy_audio_analysis("2UuKZDzjZ7cVMOvpmuAMYj") |>
  compmus_align(sections, segments) |>
  select(sections) |>
  unnest(sections) |>
  mutate(
    pitches =
      map(segments,
        compmus_summarise, pitches,
        method = "mean", norm = "manhattan"
      )
  )

```

```{r}
wgirls_chord |> 
  compmus_match_pitch_template(
    chord_templates,         # Change to chord_templates if desired
    method = "euclidean",  # Try different distance metrics
    norm = "manhattan"     # Try different norms
  ) |>
  ggplot(
    aes(x = start + duration / 2, width = duration, y = name, fill = d)
  ) +
  geom_tile() +
  scale_fill_viridis_c(guide = "none") +
  theme_minimal() +
  labs(x = "Time (s)", y = "", title = "Chordogram of 'Tell Me (2007)'")
```

### Chart C

```{r}
easy_chord <-
  get_tidy_audio_analysis("2O4Bb2WCkjlTPO827OnBMI") |>
  compmus_align(sections, segments) |>
  select(sections) |>
  unnest(sections) |>
  mutate(
    pitches =
      map(segments,
        compmus_summarise, pitches,
        method = "mean", norm = "manhattan"
      )
  )

```

```{r}
easy_chord |> 
  compmus_match_pitch_template(
    chord_templates,         # Change to chord_templates if desired
    method = "euclidean",  # Try different distance metrics
    norm = "manhattan"     # Try different norms
  ) |>
  ggplot(
    aes(x = start + duration / 2, width = duration, y = name, fill = d)
  ) +
  geom_tile() +
  scale_fill_viridis_c(guide = "none") +
  theme_minimal() +
  labs(x = "Time (s)", y = "", title = "Chordogram of 'Easy (2024)'")
```


Column 
--------------------------------------------------------------------------------

### What accounts for the prevalent use of seventh chords in K-pop songs?
Continuing the comparative analysis of 'Standing Next to You' and 'Tell Me,' I have visualized the chordograms for both tracks. In contrast to the chromagram or cepstrogram, these chordograms reveal a notable similarity in the primary chords, with seventh chords being prominently featured. For instance, 'Standing Next to You (Chart A)' incorporates chords like B:7, E:7, A:7, and D:7, mirroring the seventh chords found in 'Tell Me (Chart B),' such as B:7, D:7, Ab:7, and Gb:7.

Seventh chords, commonly found in modern music, especially jazz, play a noticeable role in both tracks. The prevalence of seventh chords in 'Standing Next to You' aligns with its jazz-funk and disco-pop influences. Similarly, 'Tell Me,' categorized as disco, extensively employs seventh chords, underscoring their significance in the genre.

To delve deeper into the relationship between seventh chords and K-pop, I examined a recent release, 'Easy (Chart C),' by LE SSERAFIM, identified as R&B trap. Surprisingly, the dominance of seventh chords persists, evident in darker lines at A:7, D:7, C:7, and F:7. This suggests a broader trend where seventh chords are commonly utilized in K-pop.


Cepstrogram
=================================================================================

Column {.tabset}
----------------------------------------------------------------------------------

### Chart A
```{r}
jungkook_timbre <-
  get_tidy_audio_analysis("2KslE17cAJNHTsI2MI0jb2") |> # Change URI.
  compmus_align(bars, segments) |>                     # Change `bars`
  select(bars) |>                                      #   in all three
  unnest(bars) |>                                      #   of these lines.
  mutate(
    pitches =
      map(segments,
        compmus_summarise, pitches,
        method = "rms", norm = "euclidean"              # Change summary & norm.
      )
  ) |>
  mutate(
    timbre =
      map(segments,
        compmus_summarise, timbre,
        method = "rms", norm = "euclidean"              # Change summary & norm.
      )
  )
```

```{r}
jungkook_timbre |>
  compmus_gather_timbre() |>
  ggplot(
    aes(
      x = start + duration / 2,
      width = duration,
      y = basis,
      fill = value
    )
  ) +
  geom_tile() +
  labs(x = "Time (s)", y = NULL, fill = "Magnitude", title = "Cepstrogram of 'Standing Next to You (2023)'") +
  scale_fill_viridis_c() +                              
  theme_classic()
```

### Chart B
```{r}
wgirls_timbre <-
  get_tidy_audio_analysis("2UuKZDzjZ7cVMOvpmuAMYj") |> # Change URI.
  compmus_align(bars, segments) |>                     # Change `bars`
  select(bars) |>                                      #   in all three
  unnest(bars) |>                                      #   of these lines.
  mutate(
    pitches =
      map(segments,
        compmus_summarise, pitches,
        method = "rms", norm = "euclidean"              # Change summary & norm.
      )
  ) |>
  mutate(
    timbre =
      map(segments,
        compmus_summarise, timbre,
        method = "rms", norm = "euclidean"              # Change summary & norm.
      )
  )
```

```{r}
wgirls_timbre |>
  compmus_gather_timbre() |>
  ggplot(
    aes(
      x = start + duration / 2,
      width = duration,
      y = basis,
      fill = value
    )
  ) +
  geom_tile() +
  labs(x = "Time (s)", y = NULL, fill = "Magnitude", title = "Cepstrogram of 'Tell Me (2007)'") +
  scale_fill_viridis_c() +                              
  theme_classic()
```


Column
----------------------------------------------------------------------------------

### Has the use of instrumentation developed since the millennium in K-pop?
In this comparison, I analyze the cepstrograms of two tracks from the playlists "K-pop ON!" and K-pop Millennium." Upon examining the visualizations, a notable distinction emerges between "Standing Next to You" and "Tell Me." The cepstrogram of "Standing Next to You" reveals a diverse magnitude spectrum, indicating in a broad range of instrumentation in use. On the other hand, "Tell Me" exhibits a consistent color frame throughout the song, indicative of a relatively uniform timbre.

Although there are several potential reasons for these observations, one could be that the advancements in musical technology over the years have enabled producers to incorporate a wider variety of sounds into their compositions. Such technological progress contributes to the diverse magnitude seen in "Standing Next to You." Moreover, the consistent timbre in "Tell Me" may be attributed to the track's relatively simple and repeating musical structure, lacking significant deviations from its main melodic line.

Corpus
==================================================================================

Column
---------------------------------------------------------------------------------

### Explanation of corpus
As an international student from South Korea, my passion for K-pop extends beyond the contemporary music of my time to include that of the preceding millennium. Over the years, I have observed the transformative changes in K-pop, influenced by advancements in production techniques and the forces of globalization. Notably, the growing collaborations between K-pop and Western artists have resulted in a fusion of diverse musical genres. In particular, I anticipate “Millennium K-pop” and “K-pop ON!” playlists to reveal distinct differences in musical components, including the tempo and rhythm, instrumentation trends, and melodic content. Upon reviewing the Spotify-curated playlists, I can conclude that both collections feature tracks that defined their respective eras. It is also important to acknowledge that both playlists maintain consistency by exclusively featuring music from K-pop groups without incorporating other genres like K-hip-hop or K-indie. However, a notable difference lies in the playlist sizes; the “Millennium K-pop” playlist consists of 100 tracks, while “K-pop ON!” features 50 tracks. For the analysis to have a more balanced comparison, it would be beneficial to augment the tracks from the millennium. In conclusion, my objective is to enhance my understanding of how K-pop has evolved by comparing the two playlists on Spotify, each representing different temporal frames - the millennium and the present.

Column
---------------------------------------------------------------------------------
<iframe src="https://open.spotify.com/embed/playlist/37i9dQZF1DWUoY6Ih7vsxr?utm_source=generator" width="100%" height="380" frameBorder="0" allowfullscreen allow="autoplay; clipboard-write; encrypted-media; fullscreen; picture-in-picture"></iframe>

Exploration
==================================================================================

Column {.tabset}
---------------------------------------------------------------------------------

### Chart A
```{r}
kpop_mill <- get_playlist_audio_features("", "2d9N60Iz8yzdsWLP1crgd2")
kpop_on <- get_playlist_audio_features("", "1y6ywhJxaBAjAQ6Ybmg0kg")
```

```{r}
kpop <-
  bind_rows(
    kpop_mill |> mutate(category = "Millennium"),
    kpop_on |> mutate(category = "ON!")
  )
```

```{r}
ggplot1 <- ggplot(kpop, aes(
  x = tempo, y = danceability, color = track.popularity, text = track.name)) + 
  geom_point() +
  scale_color_continuous(name = "Popularity", trans = "reverse") + 
  theme_light() +
  labs(title = "Relationship between Tempo and Danceability: K-pop Millennium vs. K-pop ON!", x = "Tempo", y = "Danceability") + 
  theme(plot.title = element_text(size = 10)) +
  facet_grid(. ~ playlist_name, scales = "fixed") 

ggplotly(ggplot1)
```

### Chart B
```{r}
tempo <- ggplot(kpop, aes(
  x = tempo)) + 
  geom_histogram(binwidth = 10) +
  theme_light() +
  labs(title = "Distribution of Tempo", x = "Tempo") + 
  theme(plot.title = element_text(size = 11)) + 
  facet_grid(. ~ playlist_name, scales = "fixed")
```

```{r}
danceability <- ggplot(kpop, aes(
  x = danceability)) + 
  geom_histogram(binwidth = 0.030) +
  theme_light() +
  labs(title = "Distribution of Danceability", x = "Danceability") + 
  theme(plot.title = element_text(size = 11)) + 
  facet_grid(. ~ playlist_name, scales = "fixed")
```

```{r}
grid.arrange(tempo, danceability, ncol=2)
```

Column
--------------------------------------------------------------------------------

### How does the use of pitches differ between 'Standing Next to You (2023)' and 'Tell Me (2007)?'
In exploring the relationship between danceability and tempo within the playlists "K-pop ON!" and "Millennium K-pop," it is evident that while no strong correlation exists between the two variables, danceability tends to increase with higher tempo. Additionally, the color-coded points on the scatter plot reveal that current K-pop tracks exhibit significantly higher popularity compared to tracks in the Millennium playlist. However, the correlation between track popularity and danceability remains unclear, as no definitive relationship has been established.

For a more detailed comparison between the two playlists, histograms were created to visualize the distribution of danceability and tempo. While danceability values remain relatively similar between the two playlists, there is a notable shift in tempo. In the Millennium playlist, there is a distinct preference for a specific tempo, whereas contemporary tracks display a more diverse range, resembling a more symmetrical distribution.


Chromatogram
==================================================================================

Column
----------------------------------------------------------------------------------

### Comparsion of two chromatograms

```{r}
jungkook <-
  get_tidy_audio_analysis("2KslE17cAJNHTsI2MI0jb2") |>
  select(segments) |>
  unnest(segments) |>
  select(start, duration, pitches)
```

```{r}
jungkook_chrom <- 
  jungkook |>
  mutate(pitches = map(pitches, compmus_normalise, "euclidean")) |>
  compmus_gather_chroma() |> 
  ggplot(
    aes(
      x = start + duration / 2,
      width = duration,
      y = pitch_class,
      fill = value
    )
  ) +
  geom_tile() +
  labs(title = "Chromagram of 'Standing Next to You (2023)'", x = "Time (s)", y = NULL, fill = "Magnitude") +
  theme_minimal() +
  theme(plot.title = element_text(size = 12)) + 
  scale_fill_viridis_c()
```

```{r}
wgirls <-
  get_tidy_audio_analysis("2UuKZDzjZ7cVMOvpmuAMYj") |>
  select(segments) |>
  unnest(segments) |>
  select(start, duration, pitches)
```

```{r}
wgirls_chrom <- 
  wgirls |>
  mutate(pitches = map(pitches, compmus_normalise, "euclidean")) |>
  compmus_gather_chroma() |> 
  ggplot(
    aes(
      x = start + duration / 2,
      width = duration,
      y = pitch_class,
      fill = value
    )
  ) +
  geom_tile() +
  labs(title = "Chromagram of 'Tell Me (2007)'", x = "Time (s)", y = NULL, fill = "Magnitude") +
  theme_minimal() +
  theme(plot.title = element_text(size = 12)) + 
  scale_fill_viridis_c()
```

```{r}
grid.arrange(jungkook_chrom, wgirls_chrom, ncol=1)
```

Column
--------------------------------------------------------------------------------

### How does the use of pitches differ between 'Standing Next to You (2023)' and 'Tell Me (2007)?'
I have extracted the chromagrams of two highly popular tracks from different playlists. The first chromagram represents "Standing Next to You" by Jungkook, featuring dominant notes C#/Db, D, and A. In contrast, the second chromagram depicts "Tell Me" by Wonder Girls, showing dominant notes C#/Db, F#/Gb, and B. A notable distinction emerges when comparing the two: "Standing Next to You" displays a more colorful chromagram, suggesting a diverse range of pitches and a potentially more complex harmonic structure. Furthermore, it appears that "Standing Next to You" has a clearer musical structure, with recurring patterns emphasizing its melodic motifs. It is important to note that these insights are specific to the tracks "Standing Next to You" and "Tell Me" and should not be generalized to comparisons between music from different eras, such as the millennium and the present.

Self-Similarity Matrix (SSM)
=================================================================================

Column
---------------------------------------------------------------------------------

### Comparative chromagram analysis

```{r}
cupid <-
  get_tidy_audio_analysis("5mg3VB3Qh7jcR5kAAC4DSV") |>
  select(segments) |>
  unnest(segments) |>
  select(start, duration, pitches)

cupid_sabrina <-
  get_tidy_audio_analysis("28PCjIXE4NJWDIqpRsD3rl") |>
  select(segments) |>
  unnest(segments) |>
  select(start, duration, pitches)
```

```{r}
compmus_long_distance(
  cupid |> mutate(pitches = map(pitches, compmus_normalise, "euclidean")),
  cupid_sabrina |> mutate(pitches = map(pitches, compmus_normalise, "euclidean")),
  feature = pitches,
  method = "cosine"
) |>
  ggplot(
    aes(
      x = xstart + xduration / 2,
      width = xduration,
      y = ystart + yduration / 2,
      height = yduration,
      fill = d
    )
  ) +
  geom_tile() +
  coord_equal() +
  labs(x = "Cupid - FIFTY FIFTY", y = "Cupid - FIFTY FIFTY, Sabrina Carpenter", title = "Comparative Chromagram Analysis: Two Versions of 'Cupid'") +
  theme_minimal() +
  theme(plot.title = element_text(size = 12, hjust = 0.5)) +
  scale_fill_viridis_c(guide = NULL)
```

Column
---------------------------------------------------------------------------------

### Does 'Cupid' by FIFTY FIFTY differ from 'Cupid' by FIFTY FIFTY featuring Sabrina Carpenter?
For a more in-depth analysis, I compare the chroma vectors of two different versions of the same track. On the x-axis, we have the original song, "Cupid" by FIFTY FIFTY, and on the y-axis, the remixed version featuring Sabrina Carpenter. After experimenting with various combinations of normalization and distance method, the chosen configuration involves the Euclidean normalization method paired with the cosine distance method for the final visualization. Despite the presence of a diagonal line on the graph, it lacks the intensity to indicate a strong correlation between the two versions. Additionally, the checkerboard pattern suggests potential differences between the two tracks. For instance, the remix may introduce additional chords or variations on existing chords, resulting in discrepancies in note magnitude.





